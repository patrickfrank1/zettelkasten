# DBScan Clustering Break it down for me 

url: <https://towardsdatascience.com/dbscan-clustering-break-it-down-for-me-859650a723af>
tags: clustering algorithms, dbscan, unsupervised learning

## Introduction

- acronym: density-based spatial clustering of applications with noise
- identifies clusters by data point density
    - clusters in high-density regions
    - outliers in low density regions
- 3 advantages
    - requires minimum domain knowledge
    - discovers clusters of arbitrary shape (beacuse it's all about density/connectedness)
    - efficient for large databases

## Algorithm

1. count the number of points close to each point, pints are close if they are within radius _r_
    - if _r_ is too small, then a large part of the dataset will not be clustered
    - if _r_ is too large, then clusters will merge, there will be too few clusters
2. decide which points are core points and which points are non-core points
    - parameter _minPoints_ determines if a point is a core point
    - a point is a core point if at least _minPoints_ are close to it
    - rule of thumb: _minPoints_ greater number of dimensions in dataset
3. randomly pick a core pointand assign to the first cluster
4. core points that are close to the first point are added to the cluster
5. repeat 4. until all core points that are close to a core point are added to the first cluster
6. add all non-core points that are close to the cluster, but those points are not used to extend the cluster
7. start a new cluster by performing steps 3. to 6.
8. all remaining non-core points that are not part of any cluster are called outliers

##advantages over other clustering algorithms

- KMeans and Hierarchical clustering
    - good for compact, well separated clusters
    - severely affected by noise and outliers